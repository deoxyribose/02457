\documentclass[times,12pt]{article}    % Specifies the document style.
\usepackage{amsmath,amsfonts}
\textwidth 17cm
\textheight 24cm
\oddsidemargin 0cm
\topmargin -1cm
\def\x{{\bf x}}
\def\X{{\bf X}}
\def\w{{\bf w}}
\def\t{{\bf t}}
\def\T{{\bf T}}
\def\I{{\bf I}}
\def\S{{\bf S}}
\def\m{{\bf m}}
\def\z{{\bf z}}
\def\A{\mathcal{A}}
\def\D{\mathcal{D}}
\def\squeeze{\itemsep=0pt\parskip=0pt}


\begin{document}
\section*{{\it COURSE 02457}\\[5mm] Non-Linear Signal Processing: Exercise 4}

This exercise is based on C.M.\ Bishop: {\it Pattern Recognition
and Machine Learning}, sections 1.1, 1.5.5, 3.1.1, 3.1.4, and 3.2.
The objective of the exercise is to use the MATLAB software to
illustrate and discuss the generalization concept for linear models.

Print and comment on the figures produced by the software {\sf
 main4a.m} to {\sf main4c.m} as outlined below at the three {\bf
 Checkpoints}.


\subsection*{Linear Models}
Let $y(\x)$ be a function of the vector \x, where
$\x=(x_1,\ldots,x_d)^\top$. To learn $y(\x)$  we are given a data-set, $\D = \left\{ (\x_n,t_n) \right\}$, $
n=1,\ldots,N$ of $N$ corresponding values of input (\x) and output $t$.

In this exercise we assume $y(\x)$ is a linear model
\begin{equation}
  \label{eq:sum1}
  y(\x) = w_0 + \sum_{i=1}^d w_i x_i \;\; = \;\; w_0 + \w^\top \x,
\end{equation}
where \w\ is a weight vector.

The term $w_0$ in equation~(\ref{eq:sum1}) can be included in the
weight vector, \w, by modifying \x such that
$\x=(1,x_1,\ldots,x_d)^\top$. This reduces equation~(\ref{eq:sum1}) to
\begin{equation}
  \label{eq:sum2}
  y(\x) = \sum_{i=0}^d w_i x_i \;\; = \;\; \w^\top \x.
\end{equation}

The weight-vector, \w, for the given training-set
can be estimated by minimizing an error function. Here we shall use the
sum-of-squares error function augmented by a squared weight term corresponding to a Gaussian prior 
\begin{eqnarray}
  \label{eq:error1}
  E(\w) &=& \frac{1}{2} \sum_{n=1}^N \left\{ y(\x_n;\w)-t_n \right\}^2 + \frac{1}{2}\alpha\w^2 \\
  \label{eq:error2}
       &=&  \frac{1}{2} \sum_{n=1}^N \left\{ \w^\top\x_n -t_n  \right\}^2 + \frac{1}{2}\alpha\w^2.
\end{eqnarray}
The parameter $\alpha$ is a control parameter, sometimes referred to as the "weight decay".
Introducing the matrix, \X, where $\X^\top=(\x_1\; \x_2\; \ldots
\x_N)$ and the vector, $\t = (t_1, t_2,\ldots, t_N)^\top$,
equation~(\ref{eq:error2}) can be rewritten as
\begin{equation}
  \label{eq:error3}
  E(\w) = \frac{1}{2} \left( \w^\top \X^\top \X \w + \t^\top \t -
  2\w^\top \X^\top \t  \right) + \frac{1}{2}\alpha\w^2.
\end{equation}

Since equation~(\ref{eq:error3}) is quadratic in \w, the exact value
of \w\ minimizing $E(\w)$ can be found analytically by equating the
derivative of equation~(\ref{eq:error3}) to zero\footnote{The derivative of equation~(\ref{eq:error3}) can be performed using two rules $\frac{{\partial {\z}^T {\bf Bz}}}{{\partial {\z}}} = \left( {{\bf B} + {\bf B}^T } \right){\z}
$ and $\frac{{\partial {\z}^T {\bf a}}}{{\partial {\z}}} = \frac{{\partial {\bf a}^T {\z}}}{{\partial {\z}}} =  {\bf a}$.}. This gives the
normal equations for the least-squares problem:
\begin{equation}
  \label{eq:opt}
  \left(\X^\top \X + \alpha {\bf 1}\right) \w = \X^\top \t.
\end{equation}
Where ${\bf 1}$ is a unit matrix. Solving for \w\ gives the optimal \w.
Since \X\ is an $N\times (d+1)$ matrix, $\X^\top \X$ is a $(d+1)\times
(d+1)$ square matrix. Thus the solution to equation~(\ref{eq:opt}) is
given by
\begin{equation}
  \label{eq:sol}
  \w = \left(\X^\top \X +\alpha {\bf 1}\right)^{-1} \X^\top \t.
\end{equation}

The generalization error is defined as the expectation
\begin{eqnarray}
  \label{eq:gen1}
  E_G(\w) &=& \frac{1}{2} \int\int \left\{ y(\x;\w)-t \right\}^2 p(t|\x)p(\x)dtd\x\\
  \label{eq:gen2}
       &\approx&  \frac{1}{2M} \sum_{m=1}^M \left\{ \w^\top\x_m -t_m  \right\}^2
\end{eqnarray}
approximated by the mean value over a large {\it test set} consisting of $M$ examples
drawn independently from the $N$ examples in the training set.


\subsubsection*{Checkpoint 4.1:}
Use the program {\sf main4a.m} to create a training-set with a
2-dimensional input variable and a 1-dimensional output
variable. Evaluate the training and test errors on independent sets generated by the
same true weight vector and the same noise variance, for a model with one and two
input variables respectively. In this checkpoint the weight decay is set
to zero. Compare the training and test errors {\it per example}
as function of the size of the training set. Compare the value of the training and test
errors for large training sets with the value of the noise variance.

\subsection*{Time Series Prediction}
An example where the linear model can be used is in time series
prediction. To illustrate this, consider the example of the sunspot
measurements. The number of sunspots oscillates almost periodically
over a period of some years. The average number of sunspots has been
measured yearly since 1700. Imagine we want to predict the average
number of sunspots next year. The linear model can be used for this.

Let the number of sunspots in year $n$ be $x_n$.  Let's assume that
the number of sunspots in year $n$ only depends on the number of
sunspots in the previous $d$ years. This is reasonable since there
must be a limit as to how far back one can expect a correlation. This
can be expressed as
\begin{equation}
  \label{eq:iterf}
  x_n = f(x_{n-1}, x_{n-2}, \ldots x_{n-d}).
\end{equation}
Approximating the function $f$ with a linear model gives
\begin{equation}
  \label{eq:sun1}
  x_n = w_0 + \sum_{j=1}^d w_j x_{n-j}.
\end{equation}
This corresponds to equation~(\ref{eq:sum1}), and hence is the same
problem given by equations~(\ref{eq:sum2}) to~(\ref{eq:sol}), where
the training set is given by
\begin{equation}
  \label{eq:train}
  \left.
  \begin{array}{ccl}
    \x_n &=& (1, x_{n-d}, \ldots , x_{n-1})^\top\\
    t_n &=& x_n
  \end{array} \right\} \;\; n = 1,\ldots,N-d-1.
\end{equation}
The weights can be found using equation~(\ref{eq:sol}), and the
predicted value, $x_{n+1}$, can be found from
\begin{equation}
  \label{eq:pred}
  x_{n} = y(\x_n) = \w^\top\x_n.
\end{equation}
In the context of sunspot time series prediction, the data set from 1700-1920 is
used for training while the data from 1921-1979 is used to test performance.

\subsubsection*{Checkpoint 4.2:}
Use the program {\sf main4b.m} to perform a time series prediction of
the number of sunspots with the data from 1700-1920 as training set.
Evaluate the test error on the set 1921-1979. Normalize the test error
per example by the total variance of the sunspot series.
Study the test error as function of the number of weights, $d$, (hence
years) included in the model. Which value of $d$ do you recommend?

\subsection*{Bias-variance trade-off}
The training set averages generalization error in the point $\x$
can be rewritten,
\begin{eqnarray}
{\mathbb E}_{\cal D}\left[ \left(y(\x))-{\mathbb E}_{
t}[t|\x]\right)^2 \right]
& =& {\mathbb E}_{\cal D}\left[\{y(\x)- {\mathbb E}_{\cal D}[y(\x)]\}^2\right] \nonumber\\
&+& \{{\mathbb E}_{\cal D}[y(\x)] -{\mathbb E}_{t}[t|\x]\}^2.\nonumber
\end{eqnarray}
Where ${\mathbb E}_{\cal D}$ is the expectation with respect to training sets. Note $y(\x) = y(\x;\w({\cal D}))$.

Hence the average error is split into a {\it variance} part, quantifying
 the variation among
solutions for different training sets and a {\it bias} part
quantifying the performance of the average model with respect to
best possible model ${\mathbb E}_{t}[t|\x] = \int tp(t|\x)dt$ (the conditional
mean of the output given the input).


\subsubsection*{Checkpoint 4.3:}
Use the program {\sf main4c.m} to measure the relative amount of variance and bias
for a linear model as in checkpoint 4.1 with two inputs and controlled by weight
decay. Plot the average generalization error, the bias error, and the variance
error for a large range of weight decay values. Comment on the two
regimes where the generalization error stems from variance and bias respectively.
What is the role of the weight decay in these two regimes.
Which weight decay value would you recommend?

\subsubsection*{Challenge:}
 Consider a linear model and simulated data sets of various size $N$ from a given (high dimensional) weight vector, say $d=300$, with random input vectors and additive normal noise. Plot the learning curve (test error as function of sample size) for different values of the weight decay $\alpha$.



\vspace*{1cm}
\noindent DTU, September 2009,\\[2mm]
Lars Kai Hansen, Karam Sidaros, and Carsten Stahlhut

\end{document}
